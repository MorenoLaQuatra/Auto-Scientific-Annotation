Better Arabic Parsing: Baselines, Evaluations, and Analysis Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku¨ bler, 2005). Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (§5). We quantify error categories in both evaluation settings. Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009). of Arabic. All experiments use ATB parts 1–3 divided according to the canonical split suggested by Chiang et al. We also collapse unary chains withidentical basic categories like NP → NP. Finally, we add “DT” to the tags for definite nouns and adjectives (Kulick et al., 2006). 40 75. 30 75. 85 82. Modifying the Berkeley parser for Arabic is straightforward. We use the default inference parameters. None of the models attach the attributive adjectives correctly. Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed. This paper is based on work supported in part by DARPA through IBM. evaluated