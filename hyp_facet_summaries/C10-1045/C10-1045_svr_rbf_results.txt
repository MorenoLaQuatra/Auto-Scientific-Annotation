First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2–5% F1. Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. For humans, this characteristic can impede the acquisition of literacy. How do additional ambiguities caused by devocalization affect statistical learning? Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase. For parsing, the most challenging form of ambiguity occurs at the discourse level. English parsing evaluations usually report results on sentences up to length 40. segmentation (Table 2). 3.1 Gross Statistics. In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance. 3.2 Inter-annotator Agreement. As a result, Habash et al. Table 6: Incremental dev set results for the manually annotated grammar (sentences of length ≤ 70). This feature has a linguistic justification. But for eign learners are often surprised by the verbless predications that are frequently used in Arabic. Lexicalizing several POS tags improves performance. splitIN captures the verb/preposition idioms that are widespread in Arabic. For example, we might have VP → VB NP PP, where the NP is the subject. (2006). At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP → NP. In Table 7 we give results for several evaluation metrics. splitPUNC restores the convention of the WSJ. 58 95. 49 99. 92 76. 00 76. 95 76. 96 75. 68 96. 40 75. 30 75. 85 82. 3. Consequently, all three parsers prefer the nominal reading. None of the models attach the attributive adjectives correctly. evaluated