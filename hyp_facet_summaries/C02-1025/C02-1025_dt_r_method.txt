Recently, statistical NERs have achieved results that are comparable to hand-coded systems. Mikheev et al. A token that is allCaps will also be initCaps. At most one feature in this group will be set to 1. If is not initCaps, then (not-initCaps, ) is set to 1. For example, if is found in the list of person first names, the feature PersonFirstName is set to 1. Two lists, Corporate-Suffix-List (for corporate suffixes) and Person-Prefix-List (for person prefixes), are collected from the training data. Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr., etc are not part of person names, whereas corporate suffixes like Corp., Inc., etc are part of corporate names. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. ‚Äù). For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears. MENE has only been tested on MUC7. Mikheev et al. We have shown that the maximum entropy framework is able to use global information directly. Otherwise,