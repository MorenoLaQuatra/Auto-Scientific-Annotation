It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Mikheev et al. MENE is then trained on 80% of the training corpus, and tested on the remaining 20%. both MENE and IdentiFinder used more training data than we did (we used only the official MUC 6 and MUC7 training data). 3.1 Maximum Entropy. Such constraints are derived from training data, expressing some relationship between features and outcome. For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972). 4.1 Local Features. At most one feature in this group will be set to 1. For all lists except locations, the lists are processed into a list of tokens (unigrams). Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears. ICOC and CSPP contributed the greatest im provements. All our results are obtained by using only the official training data provided by the MUC conferences. Mikheev et al. Both